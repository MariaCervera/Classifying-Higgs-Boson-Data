{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Higgs Boson Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import costs\n",
    "import datetime\n",
    "from helpers import *\n",
    "from optimization import *\n",
    "from proj1_helpers import *\n",
    "from crossvalidation import *\n",
    "from to_create_pred import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../../Data/train.csv' \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tx, mean_x, std_x = standardize(x)\n",
    "# We standardize the data because gradient descent is very sensitive to different speed of convergence\n",
    "DATA_TEST_PATH = '../../Data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tx_test, mean_x, std_x = standardize(tX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) We try our models without any improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Least Squares with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried for lambdas np.logspace(-5, 0, 10) and it seems like the optimal for degree 1 polynomial basis is in [0.1;0.17["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for lamb =  0.11  loss_tr =  0.824866355769  loss_te =  0.824715937735\n",
      "for lamb =  0.11291549665  loss_tr =  0.824859207669  loss_te =  0.824709105286\n",
      "for lamb =  0.116681005372  loss_tr =  0.824851503289  loss_te =  0.824701783923\n",
      "for lamb =  0.1215443469  loss_tr =  0.824843609509  loss_te =  0.824694346624\n",
      "for lamb =  0.127825594022  loss_tr =  0.824836021446  loss_te =  0.824687290248\n",
      "for lamb =  0.135938136638  loss_tr =  0.824829282935  loss_te =  0.824681153227\n",
      "for lamb =  0.146415888336  loss_tr =  0.824823846746  loss_te =  0.824676373042\n",
      "for lamb =  0.159948425032  loss_tr =  0.824819920804  loss_te =  0.82467312327\n",
      "for lamb =  0.177426368268  loss_tr =  8.02072229935e+20  loss_te =  8.01272084026e+20\n",
      "Higher values of lambda will result in high divergence yielding infinite loss\n",
      "The optimal lambda for GD is :  0.159948425032  ; loss_tr =  0.824819920804  ; loss_te =  0.82467312327\n"
     ]
    }
   ],
   "source": [
    "optimal_lambda,optimal_loss_tr,optimal_loss_te = get_best_parameters_GD(y,tx)\n",
    "print(\"The optimal lambda for GD is : \",optimal_lambda,\" ; loss_tr = \",optimal_loss_tr,\" ; loss_te = \",optimal_loss_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = least_squares_GD(y,tx,optimal_lambda,300)\n",
    "generate_outputs_for_weigths(weights,tx_test,ids_test,'LeastSquaresGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Least squares with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried for lambdas np.logspace(-5, 0, 10) and it seems like the optimal for degree 1 polynomial basis is in [0.001;0.01["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for lamb =  0.0021  loss_tr =  0.874438662643  loss_te =  0.874610956965\n",
      "for lamb =  0.0021291549665  loss_tr =  0.879946104725  loss_te =  0.879932031232\n",
      "for lamb =  0.00216681005372  loss_tr =  0.874014019209  loss_te =  0.873806336215\n",
      "for lamb =  0.002215443469  loss_tr =  0.872443546047  loss_te =  0.872736720464\n",
      "for lamb =  0.00227825594022  loss_tr =  0.874879572911  loss_te =  0.874799806263\n",
      "for lamb =  0.00235938136638  loss_tr =  0.880989074319  loss_te =  0.881151025915\n",
      "for lamb =  0.00246415888336  loss_tr =  0.873610518604  loss_te =  0.873196191409\n",
      "for lamb =  0.00259948425032  loss_tr =  0.874955238278  loss_te =  0.87570775559\n",
      "for lamb =  0.00277426368268  loss_tr =  0.878585853626  loss_te =  0.878800302355\n",
      "for lamb =  0.003  loss_tr =  0.876667728946  loss_te =  0.87670104004\n",
      "The optimal lambda for GD is :  0.002215443469  ; loss_tr =  0.872443546047  ; loss_te =  0.872736720464\n"
     ]
    }
   ],
   "source": [
    "optimal_lambda,optimal_loss_tr,optimal_loss_te = get_best_parameters_SGD(y,tx)\n",
    "print(\"The optimal lambda for GD is : \",optimal_lambda,\" ; loss_tr = \",optimal_loss_tr,\" ; loss_te = \",optimal_loss_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = least_squares_SGD(y,tx,optimal_lambda,500)\n",
    "generate_outputs_for_weigths(weights,tx_test,ids_test,'LeastSquaresSGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to simply get the same unit of comparison with gradient descent methods, we see that Least Squares yields a much better loss for the test sample on cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss_tr =  0.824104577101  loss_te =  0.823974545767\n"
     ]
    }
   ],
   "source": [
    "cross_validate_Least_Squares(y,tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = least_squares(y,tx)\n",
    "generate_outputs_for_weigths(weights,tx_test,ids_test,'LeastSquares')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should improve least squares to get less complex models. Tried on np.logspace(-5,2,10) and it seems like the optimal value is in [0.1;0.11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal lambda for GD is :  0.19  ; loss_tr =  0.824399393203  ; loss_te =  0.824266959013\n"
     ]
    }
   ],
   "source": [
    "optimal_lambda,optimal_loss_tr,optimal_loss_te = get_best_parameters_Ridge(y,x)\n",
    "print(\"The optimal lambda for GD is : \",optimal_lambda,\" ; loss_tr = \",optimal_loss_tr,\" ; loss_te = \",optimal_loss_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = ridge_regression(y,tx,optimal_lambda)\n",
    "generate_outputs_for_weigths(weights,tx_test,ids_test,'RidgeRegression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with my ridge regression as it seems that the cost function goes into the negative value, and it should not ? \n",
    "I checked the gradient and cost computation for small values of y, tx and w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=69314.71805599453\n",
      "Current iteration=500, the loss=84915.74781887252\n",
      "Current iteration=1000, the loss=84857.51961439503\n",
      "Current iteration=1500, the loss=84858.08767789285\n",
      "Current iteration=2000, the loss=84858.29400979228\n",
      "Current iteration=2500, the loss=84858.38375005157\n",
      "Current iteration=3000, the loss=84858.45024057636\n",
      "Current iteration=3500, the loss=84858.50672900704\n",
      "Current iteration=4000, the loss=84858.55528440129\n",
      "Current iteration=4500, the loss=84858.59647292414\n",
      "Current iteration=5000, the loss=84858.63064902411\n",
      "Current iteration=5500, the loss=84858.65812747812\n",
      "Current iteration=6000, the loss=84858.67920671559\n",
      "Current iteration=6500, the loss=84858.69417237196\n",
      "Current iteration=7000, the loss=84858.70329820046\n",
      "Current iteration=7500, the loss=84858.70684661104\n",
      "Current iteration=8000, the loss=84858.70506914471\n",
      "Current iteration=8500, the loss=84858.69820692108\n",
      "Current iteration=9000, the loss=84858.68649106866\n",
      "Current iteration=9500, the loss=84858.67014313495\n"
     ]
    }
   ],
   "source": [
    "w = logistic_regression(y[:100000],tx[:100000],0.00002,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=144403.3521260534\n",
      "Current iteration=1, the loss=inf\n",
      "Current iteration=2, the loss=nan\n",
      "Current iteration=3, the loss=nan\n",
      "Current iteration=4, the loss=nan\n",
      "Current iteration=5, the loss=nan\n",
      "Current iteration=6, the loss=nan\n",
      "Current iteration=7, the loss=nan\n",
      "Current iteration=8, the loss=nan\n",
      "Current iteration=9, the loss=nan\n",
      "Current iteration=10, the loss=nan\n",
      "Current iteration=11, the loss=nan\n",
      "Current iteration=12, the loss=nan\n",
      "Current iteration=13, the loss=nan\n",
      "Current iteration=14, the loss=nan\n",
      "Current iteration=15, the loss=nan\n",
      "Current iteration=16, the loss=nan\n",
      "Current iteration=17, the loss=nan\n",
      "Current iteration=18, the loss=nan\n",
      "Current iteration=19, the loss=nan\n",
      "Current iteration=20, the loss=nan\n",
      "Current iteration=21, the loss=nan\n",
      "Current iteration=22, the loss=nan\n",
      "Current iteration=23, the loss=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugomoreau/Dropbox/Etudes/Master/Pattern classification and machine learning/Projects/Project 1/Github-Project1-PCML/scripts/optimization.py:160: RuntimeWarning: overflow encountered in exp\n",
      "  diff = np.log(1+np.exp(tmp)) - y*tmp\n",
      "/Users/hugomoreau/Dropbox/Etudes/Master/Pattern classification and machine learning/Projects/Project 1/Github-Project1-PCML/scripts/optimization.py:155: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(t) / (1 + np.exp(t))\n",
      "/Users/hugomoreau/Dropbox/Etudes/Master/Pattern classification and machine learning/Projects/Project 1/Github-Project1-PCML/scripts/optimization.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(t) / (1 + np.exp(t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=24, the loss=nan\n",
      "Current iteration=25, the loss=nan\n",
      "Current iteration=26, the loss=nan\n",
      "Current iteration=27, the loss=nan\n",
      "Current iteration=28, the loss=nan\n",
      "Current iteration=29, the loss=nan\n",
      "Current iteration=30, the loss=nan\n",
      "Current iteration=31, the loss=nan\n",
      "Current iteration=32, the loss=nan\n",
      "Current iteration=33, the loss=nan\n",
      "Current iteration=34, the loss=nan\n",
      "Current iteration=35, the loss=nan\n",
      "Current iteration=36, the loss=nan\n",
      "Current iteration=37, the loss=nan\n",
      "Current iteration=38, the loss=nan\n",
      "Current iteration=39, the loss=nan\n",
      "Current iteration=40, the loss=nan\n",
      "Current iteration=41, the loss=nan\n",
      "Current iteration=42, the loss=nan\n",
      "Current iteration=43, the loss=nan\n",
      "Current iteration=44, the loss=nan\n",
      "Current iteration=45, the loss=nan\n",
      "Current iteration=46, the loss=nan\n",
      "Current iteration=47, the loss=nan\n",
      "Current iteration=48, the loss=nan\n",
      "Current iteration=49, the loss=nan\n",
      "Current iteration=50, the loss=nan\n",
      "Current iteration=51, the loss=nan\n",
      "Current iteration=52, the loss=nan\n",
      "Current iteration=53, the loss=nan\n",
      "Current iteration=54, the loss=nan\n",
      "Current iteration=55, the loss=nan\n",
      "Current iteration=56, the loss=nan\n",
      "Current iteration=57, the loss=nan\n",
      "Current iteration=58, the loss=nan\n",
      "Current iteration=59, the loss=nan\n",
      "Current iteration=60, the loss=nan\n",
      "Current iteration=61, the loss=nan\n",
      "Current iteration=62, the loss=nan\n",
      "Current iteration=63, the loss=nan\n",
      "Current iteration=64, the loss=nan\n",
      "Current iteration=65, the loss=nan\n",
      "Current iteration=66, the loss=nan\n",
      "Current iteration=67, the loss=nan\n",
      "Current iteration=68, the loss=nan\n",
      "Current iteration=69, the loss=nan\n",
      "Current iteration=70, the loss=nan\n",
      "Current iteration=71, the loss=nan\n",
      "Current iteration=72, the loss=nan\n",
      "Current iteration=73, the loss=nan\n",
      "Current iteration=74, the loss=nan\n",
      "Current iteration=75, the loss=nan\n",
      "Current iteration=76, the loss=nan\n",
      "Current iteration=77, the loss=nan\n",
      "Current iteration=78, the loss=nan\n",
      "Current iteration=79, the loss=nan\n",
      "Current iteration=80, the loss=nan\n",
      "Current iteration=81, the loss=nan\n",
      "Current iteration=82, the loss=nan\n",
      "Current iteration=83, the loss=nan\n",
      "Current iteration=84, the loss=nan\n",
      "Current iteration=85, the loss=nan\n",
      "Current iteration=86, the loss=nan\n",
      "Current iteration=87, the loss=nan\n",
      "Current iteration=88, the loss=nan\n",
      "Current iteration=89, the loss=nan\n",
      "Current iteration=90, the loss=nan\n",
      "Current iteration=91, the loss=nan\n",
      "Current iteration=92, the loss=nan\n",
      "Current iteration=93, the loss=nan\n",
      "Current iteration=94, the loss=nan\n",
      "Current iteration=95, the loss=nan\n",
      "Current iteration=96, the loss=nan\n",
      "Current iteration=97, the loss=nan\n",
      "Current iteration=98, the loss=nan\n",
      "Current iteration=99, the loss=nan\n",
      "Current iteration=100, the loss=nan\n",
      "Current iteration=101, the loss=nan\n",
      "Current iteration=102, the loss=nan\n",
      "Current iteration=103, the loss=nan\n",
      "Current iteration=104, the loss=nan\n",
      "Current iteration=105, the loss=nan\n",
      "Current iteration=106, the loss=nan\n",
      "Current iteration=107, the loss=nan\n",
      "Current iteration=108, the loss=nan\n",
      "Current iteration=109, the loss=nan\n",
      "Current iteration=110, the loss=nan\n",
      "Current iteration=111, the loss=nan\n",
      "Current iteration=112, the loss=nan\n",
      "Current iteration=113, the loss=nan\n",
      "Current iteration=114, the loss=nan\n",
      "Current iteration=115, the loss=nan\n",
      "Current iteration=116, the loss=nan\n",
      "Current iteration=117, the loss=nan\n",
      "Current iteration=118, the loss=nan\n",
      "Current iteration=119, the loss=nan\n",
      "Current iteration=120, the loss=nan\n",
      "Current iteration=121, the loss=nan\n",
      "Current iteration=122, the loss=nan\n",
      "Current iteration=123, the loss=nan\n",
      "Current iteration=124, the loss=nan\n",
      "Current iteration=125, the loss=nan\n",
      "Current iteration=126, the loss=nan\n",
      "Current iteration=127, the loss=nan\n",
      "Current iteration=128, the loss=nan\n",
      "Current iteration=129, the loss=nan\n",
      "Current iteration=130, the loss=nan\n",
      "Current iteration=131, the loss=nan\n",
      "Current iteration=132, the loss=nan\n",
      "Current iteration=133, the loss=nan\n",
      "Current iteration=134, the loss=nan\n",
      "Current iteration=135, the loss=nan\n",
      "Current iteration=136, the loss=nan\n",
      "Current iteration=137, the loss=nan\n",
      "Current iteration=138, the loss=nan\n",
      "Current iteration=139, the loss=nan\n",
      "Current iteration=140, the loss=nan\n",
      "Current iteration=141, the loss=nan\n",
      "Current iteration=142, the loss=nan\n",
      "Current iteration=143, the loss=nan\n",
      "Current iteration=144, the loss=nan\n",
      "Current iteration=145, the loss=nan\n",
      "Current iteration=146, the loss=nan\n",
      "Current iteration=147, the loss=nan\n",
      "Current iteration=148, the loss=nan\n",
      "Current iteration=149, the loss=nan\n",
      "Current iteration=150, the loss=nan\n",
      "Current iteration=151, the loss=nan\n",
      "Current iteration=152, the loss=nan\n",
      "Current iteration=153, the loss=nan\n",
      "Current iteration=154, the loss=nan\n",
      "Current iteration=155, the loss=nan\n",
      "Current iteration=156, the loss=nan\n",
      "Current iteration=157, the loss=nan\n",
      "Current iteration=158, the loss=nan\n",
      "Current iteration=159, the loss=nan\n",
      "Current iteration=160, the loss=nan\n",
      "Current iteration=161, the loss=nan\n",
      "Current iteration=162, the loss=nan\n",
      "Current iteration=163, the loss=nan\n",
      "Current iteration=164, the loss=nan\n",
      "Current iteration=165, the loss=nan\n",
      "Current iteration=166, the loss=nan\n",
      "Current iteration=167, the loss=nan\n",
      "Current iteration=168, the loss=nan\n",
      "Current iteration=169, the loss=nan\n",
      "Current iteration=170, the loss=nan\n",
      "Current iteration=171, the loss=nan\n",
      "Current iteration=172, the loss=nan\n",
      "Current iteration=173, the loss=nan\n",
      "Current iteration=174, the loss=nan\n",
      "Current iteration=175, the loss=nan\n",
      "Current iteration=176, the loss=nan\n",
      "Current iteration=177, the loss=nan\n",
      "Current iteration=178, the loss=nan\n",
      "Current iteration=179, the loss=nan\n",
      "Current iteration=180, the loss=nan\n",
      "Current iteration=181, the loss=nan\n",
      "Current iteration=182, the loss=nan\n",
      "Current iteration=183, the loss=nan\n",
      "Current iteration=184, the loss=nan\n",
      "Current iteration=185, the loss=nan\n",
      "Current iteration=186, the loss=nan\n",
      "Current iteration=187, the loss=nan\n",
      "Current iteration=188, the loss=nan\n",
      "Current iteration=189, the loss=nan\n",
      "Current iteration=190, the loss=nan\n",
      "Current iteration=191, the loss=nan\n",
      "Current iteration=192, the loss=nan\n",
      "Current iteration=193, the loss=nan\n",
      "Current iteration=194, the loss=nan\n",
      "Current iteration=195, the loss=nan\n",
      "Current iteration=196, the loss=nan\n",
      "Current iteration=197, the loss=nan\n",
      "Current iteration=198, the loss=nan\n",
      "Current iteration=199, the loss=nan\n",
      "Current iteration=200, the loss=nan\n",
      "Current iteration=201, the loss=nan\n",
      "Current iteration=202, the loss=nan\n",
      "Current iteration=203, the loss=nan\n",
      "Current iteration=204, the loss=nan\n",
      "Current iteration=205, the loss=nan\n",
      "Current iteration=206, the loss=nan\n",
      "Current iteration=207, the loss=nan\n",
      "Current iteration=208, the loss=nan\n",
      "Current iteration=209, the loss=nan\n",
      "Current iteration=210, the loss=nan\n",
      "Current iteration=211, the loss=nan\n",
      "Current iteration=212, the loss=nan\n",
      "Current iteration=213, the loss=nan\n",
      "Current iteration=214, the loss=nan\n",
      "Current iteration=215, the loss=nan\n",
      "Current iteration=216, the loss=nan\n",
      "Current iteration=217, the loss=nan\n",
      "Current iteration=218, the loss=nan\n",
      "Current iteration=219, the loss=nan\n",
      "Current iteration=220, the loss=nan\n",
      "Current iteration=221, the loss=nan\n",
      "Current iteration=222, the loss=nan\n",
      "Current iteration=223, the loss=nan\n",
      "Current iteration=224, the loss=nan\n",
      "Current iteration=225, the loss=nan\n",
      "Current iteration=226, the loss=nan\n",
      "Current iteration=227, the loss=nan\n",
      "Current iteration=228, the loss=nan\n",
      "Current iteration=229, the loss=nan\n",
      "Current iteration=230, the loss=nan\n",
      "Current iteration=231, the loss=nan\n",
      "Current iteration=232, the loss=nan\n",
      "Current iteration=233, the loss=nan\n",
      "Current iteration=234, the loss=nan\n",
      "Current iteration=235, the loss=nan\n",
      "Current iteration=236, the loss=nan\n",
      "Current iteration=237, the loss=nan\n",
      "Current iteration=238, the loss=nan\n",
      "Current iteration=239, the loss=nan\n",
      "Current iteration=240, the loss=nan\n",
      "Current iteration=241, the loss=nan\n",
      "Current iteration=242, the loss=nan\n",
      "Current iteration=243, the loss=nan\n",
      "Current iteration=244, the loss=nan\n",
      "Current iteration=245, the loss=nan\n",
      "Current iteration=246, the loss=nan\n",
      "Current iteration=247, the loss=nan\n",
      "Current iteration=248, the loss=nan\n",
      "Current iteration=249, the loss=nan\n",
      "Current iteration=250, the loss=nan\n",
      "Current iteration=251, the loss=nan\n",
      "Current iteration=252, the loss=nan\n",
      "Current iteration=253, the loss=nan\n",
      "Current iteration=254, the loss=nan\n",
      "Current iteration=255, the loss=nan\n",
      "Current iteration=256, the loss=nan\n",
      "Current iteration=257, the loss=nan\n",
      "Current iteration=258, the loss=nan\n",
      "Current iteration=259, the loss=nan\n",
      "Current iteration=260, the loss=nan\n",
      "Current iteration=261, the loss=nan\n",
      "Current iteration=262, the loss=nan\n",
      "Current iteration=263, the loss=nan\n",
      "Current iteration=264, the loss=nan\n",
      "Current iteration=265, the loss=nan\n",
      "Current iteration=266, the loss=nan\n",
      "Current iteration=267, the loss=nan\n",
      "Current iteration=268, the loss=nan\n",
      "Current iteration=269, the loss=nan\n",
      "Current iteration=270, the loss=nan\n",
      "Current iteration=271, the loss=nan\n",
      "Current iteration=272, the loss=nan\n",
      "Current iteration=273, the loss=nan\n",
      "Current iteration=274, the loss=nan\n",
      "Current iteration=275, the loss=nan\n",
      "Current iteration=276, the loss=nan\n",
      "Current iteration=277, the loss=nan\n",
      "Current iteration=278, the loss=nan\n",
      "Current iteration=279, the loss=nan\n",
      "Current iteration=280, the loss=nan\n",
      "Current iteration=281, the loss=nan\n",
      "Current iteration=282, the loss=nan\n",
      "Current iteration=283, the loss=nan\n",
      "Current iteration=284, the loss=nan\n",
      "Current iteration=285, the loss=nan\n",
      "Current iteration=286, the loss=nan\n",
      "Current iteration=287, the loss=nan\n",
      "Current iteration=288, the loss=nan\n",
      "Current iteration=289, the loss=nan\n",
      "Current iteration=290, the loss=nan\n",
      "Current iteration=291, the loss=nan\n",
      "Current iteration=292, the loss=nan\n",
      "Current iteration=293, the loss=nan\n",
      "Current iteration=294, the loss=nan\n",
      "Current iteration=295, the loss=nan\n",
      "Current iteration=296, the loss=nan\n",
      "Current iteration=297, the loss=nan\n",
      "Current iteration=298, the loss=nan\n",
      "Current iteration=299, the loss=nan\n",
      "Current iteration=300, the loss=nan\n",
      "Current iteration=301, the loss=nan\n",
      "Current iteration=302, the loss=nan\n",
      "Current iteration=303, the loss=nan\n",
      "Current iteration=304, the loss=nan\n",
      "Current iteration=305, the loss=nan\n",
      "Current iteration=306, the loss=nan\n",
      "Current iteration=307, the loss=nan\n",
      "Current iteration=308, the loss=nan\n",
      "Current iteration=309, the loss=nan\n",
      "Current iteration=310, the loss=nan\n",
      "Current iteration=311, the loss=nan\n",
      "Current iteration=312, the loss=nan\n",
      "Current iteration=313, the loss=nan\n",
      "Current iteration=314, the loss=nan\n",
      "Current iteration=315, the loss=nan\n",
      "Current iteration=316, the loss=nan\n",
      "Current iteration=317, the loss=nan\n",
      "Current iteration=318, the loss=nan\n",
      "Current iteration=319, the loss=nan\n",
      "Current iteration=320, the loss=nan\n",
      "Current iteration=321, the loss=nan\n",
      "Current iteration=322, the loss=nan\n",
      "Current iteration=323, the loss=nan\n",
      "Current iteration=324, the loss=nan\n",
      "Current iteration=325, the loss=nan\n",
      "Current iteration=326, the loss=nan\n",
      "Current iteration=327, the loss=nan\n",
      "Current iteration=328, the loss=nan\n",
      "Current iteration=329, the loss=nan\n",
      "Current iteration=330, the loss=nan\n",
      "Current iteration=331, the loss=nan\n",
      "Current iteration=332, the loss=nan\n",
      "Current iteration=333, the loss=nan\n",
      "Current iteration=334, the loss=nan\n",
      "Current iteration=335, the loss=nan\n",
      "Current iteration=336, the loss=nan\n",
      "Current iteration=337, the loss=nan\n",
      "Current iteration=338, the loss=nan\n",
      "Current iteration=339, the loss=nan\n",
      "Current iteration=340, the loss=nan\n",
      "Current iteration=341, the loss=nan\n",
      "Current iteration=342, the loss=nan\n",
      "Current iteration=343, the loss=nan\n",
      "Current iteration=344, the loss=nan\n",
      "Current iteration=345, the loss=nan\n",
      "Current iteration=346, the loss=nan\n",
      "Current iteration=347, the loss=nan\n",
      "Current iteration=348, the loss=nan\n",
      "Current iteration=349, the loss=nan\n",
      "Current iteration=350, the loss=nan\n",
      "Current iteration=351, the loss=nan\n",
      "Current iteration=352, the loss=nan\n",
      "Current iteration=353, the loss=nan\n",
      "Current iteration=354, the loss=nan\n",
      "Current iteration=355, the loss=nan\n",
      "Current iteration=356, the loss=nan\n",
      "Current iteration=357, the loss=nan\n",
      "Current iteration=358, the loss=nan\n",
      "Current iteration=359, the loss=nan\n",
      "Current iteration=360, the loss=nan\n",
      "Current iteration=361, the loss=nan\n",
      "Current iteration=362, the loss=nan\n",
      "Current iteration=363, the loss=nan\n",
      "Current iteration=364, the loss=nan\n",
      "Current iteration=365, the loss=nan\n",
      "Current iteration=366, the loss=nan\n",
      "Current iteration=367, the loss=nan\n",
      "Current iteration=368, the loss=nan\n",
      "Current iteration=369, the loss=nan\n",
      "Current iteration=370, the loss=nan\n",
      "Current iteration=371, the loss=nan\n",
      "Current iteration=372, the loss=nan\n",
      "Current iteration=373, the loss=nan\n",
      "Current iteration=374, the loss=nan\n",
      "Current iteration=375, the loss=nan\n",
      "Current iteration=376, the loss=nan\n",
      "Current iteration=377, the loss=nan\n",
      "Current iteration=378, the loss=nan\n",
      "Current iteration=379, the loss=nan\n",
      "Current iteration=380, the loss=nan\n",
      "Current iteration=381, the loss=nan\n",
      "Current iteration=382, the loss=nan\n",
      "Current iteration=383, the loss=nan\n",
      "Current iteration=384, the loss=nan\n",
      "Current iteration=385, the loss=nan\n",
      "Current iteration=386, the loss=nan\n",
      "Current iteration=387, the loss=nan\n",
      "Current iteration=388, the loss=nan\n",
      "Current iteration=389, the loss=nan\n",
      "Current iteration=390, the loss=nan\n",
      "Current iteration=391, the loss=nan\n",
      "Current iteration=392, the loss=nan\n",
      "Current iteration=393, the loss=nan\n",
      "Current iteration=394, the loss=nan\n",
      "Current iteration=395, the loss=nan\n",
      "Current iteration=396, the loss=nan\n",
      "Current iteration=397, the loss=nan\n",
      "Current iteration=398, the loss=nan\n",
      "Current iteration=399, the loss=nan\n",
      "Current iteration=400, the loss=nan\n",
      "Current iteration=401, the loss=nan\n",
      "Current iteration=402, the loss=nan\n",
      "Current iteration=403, the loss=nan\n",
      "Current iteration=404, the loss=nan\n",
      "Current iteration=405, the loss=nan\n",
      "Current iteration=406, the loss=nan\n",
      "Current iteration=407, the loss=nan\n",
      "Current iteration=408, the loss=nan\n",
      "Current iteration=409, the loss=nan\n",
      "Current iteration=410, the loss=nan\n",
      "Current iteration=411, the loss=nan\n",
      "Current iteration=412, the loss=nan\n",
      "Current iteration=413, the loss=nan\n",
      "Current iteration=414, the loss=nan\n",
      "Current iteration=415, the loss=nan\n",
      "Current iteration=416, the loss=nan\n",
      "Current iteration=417, the loss=nan\n",
      "Current iteration=418, the loss=nan\n",
      "Current iteration=419, the loss=nan\n",
      "Current iteration=420, the loss=nan\n",
      "Current iteration=421, the loss=nan\n",
      "Current iteration=422, the loss=nan\n",
      "Current iteration=423, the loss=nan\n",
      "Current iteration=424, the loss=nan\n",
      "Current iteration=425, the loss=nan\n",
      "Current iteration=426, the loss=nan\n",
      "Current iteration=427, the loss=nan\n",
      "Current iteration=428, the loss=nan\n",
      "Current iteration=429, the loss=nan\n",
      "Current iteration=430, the loss=nan\n",
      "Current iteration=431, the loss=nan\n",
      "Current iteration=432, the loss=nan\n",
      "Current iteration=433, the loss=nan\n",
      "Current iteration=434, the loss=nan\n",
      "Current iteration=435, the loss=nan\n",
      "Current iteration=436, the loss=nan\n",
      "Current iteration=437, the loss=nan\n",
      "Current iteration=438, the loss=nan\n",
      "Current iteration=439, the loss=nan\n",
      "Current iteration=440, the loss=nan\n",
      "Current iteration=441, the loss=nan\n",
      "Current iteration=442, the loss=nan\n",
      "Current iteration=443, the loss=nan\n",
      "Current iteration=444, the loss=nan\n",
      "Current iteration=445, the loss=nan\n",
      "Current iteration=446, the loss=nan\n",
      "Current iteration=447, the loss=nan\n",
      "Current iteration=448, the loss=nan\n",
      "Current iteration=449, the loss=nan\n",
      "Current iteration=450, the loss=nan\n",
      "Current iteration=451, the loss=nan\n",
      "Current iteration=452, the loss=nan\n",
      "Current iteration=453, the loss=nan\n",
      "Current iteration=454, the loss=nan\n",
      "Current iteration=455, the loss=nan\n",
      "Current iteration=456, the loss=nan\n",
      "Current iteration=457, the loss=nan\n",
      "Current iteration=458, the loss=nan\n",
      "Current iteration=459, the loss=nan\n",
      "Current iteration=460, the loss=nan\n",
      "Current iteration=461, the loss=nan\n",
      "Current iteration=462, the loss=nan\n",
      "Current iteration=463, the loss=nan\n",
      "Current iteration=464, the loss=nan\n",
      "Current iteration=465, the loss=nan\n",
      "Current iteration=466, the loss=nan\n",
      "Current iteration=467, the loss=nan\n",
      "Current iteration=468, the loss=nan\n",
      "Current iteration=469, the loss=nan\n",
      "Current iteration=470, the loss=nan\n",
      "Current iteration=471, the loss=nan\n",
      "Current iteration=472, the loss=nan\n",
      "Current iteration=473, the loss=nan\n",
      "Current iteration=474, the loss=nan\n",
      "Current iteration=475, the loss=nan\n",
      "Current iteration=476, the loss=nan\n",
      "Current iteration=477, the loss=nan\n",
      "Current iteration=478, the loss=nan\n",
      "Current iteration=479, the loss=nan\n",
      "Current iteration=480, the loss=nan\n",
      "Current iteration=481, the loss=nan\n",
      "Current iteration=482, the loss=nan\n",
      "Current iteration=483, the loss=nan\n",
      "Current iteration=484, the loss=nan\n",
      "Current iteration=485, the loss=nan\n",
      "Current iteration=486, the loss=nan\n",
      "Current iteration=487, the loss=nan\n",
      "Current iteration=488, the loss=nan\n",
      "Current iteration=489, the loss=nan\n",
      "Current iteration=490, the loss=nan\n",
      "Current iteration=491, the loss=nan\n",
      "Current iteration=492, the loss=nan\n",
      "Current iteration=493, the loss=nan\n",
      "Current iteration=494, the loss=nan\n",
      "Current iteration=495, the loss=nan\n",
      "Current iteration=496, the loss=nan\n",
      "Current iteration=497, the loss=nan\n",
      "Current iteration=498, the loss=nan\n",
      "Current iteration=499, the loss=nan\n",
      "Current iteration=500, the loss=nan\n",
      "Current iteration=501, the loss=nan\n",
      "Current iteration=502, the loss=nan\n",
      "Current iteration=503, the loss=nan\n",
      "Current iteration=504, the loss=nan\n",
      "Current iteration=505, the loss=nan\n",
      "Current iteration=506, the loss=nan\n",
      "Current iteration=507, the loss=nan\n",
      "Current iteration=508, the loss=nan\n",
      "Current iteration=509, the loss=nan\n",
      "Current iteration=510, the loss=nan\n",
      "Current iteration=511, the loss=nan\n",
      "Current iteration=512, the loss=nan\n",
      "Current iteration=513, the loss=nan\n",
      "Current iteration=514, the loss=nan\n",
      "Current iteration=515, the loss=nan\n",
      "Current iteration=516, the loss=nan\n",
      "Current iteration=517, the loss=nan\n",
      "Current iteration=518, the loss=nan\n",
      "Current iteration=519, the loss=nan\n",
      "Current iteration=520, the loss=nan\n",
      "Current iteration=521, the loss=nan\n",
      "Current iteration=522, the loss=nan\n",
      "Current iteration=523, the loss=nan\n",
      "Current iteration=524, the loss=nan\n",
      "Current iteration=525, the loss=nan\n",
      "Current iteration=526, the loss=nan\n",
      "Current iteration=527, the loss=nan\n",
      "Current iteration=528, the loss=nan\n",
      "Current iteration=529, the loss=nan\n",
      "Current iteration=530, the loss=nan\n",
      "Current iteration=531, the loss=nan\n",
      "Current iteration=532, the loss=nan\n",
      "Current iteration=533, the loss=nan\n",
      "Current iteration=534, the loss=nan\n",
      "Current iteration=535, the loss=nan\n",
      "Current iteration=536, the loss=nan\n",
      "Current iteration=537, the loss=nan\n",
      "Current iteration=538, the loss=nan\n",
      "Current iteration=539, the loss=nan\n",
      "Current iteration=540, the loss=nan\n",
      "Current iteration=541, the loss=nan\n",
      "Current iteration=542, the loss=nan\n",
      "Current iteration=543, the loss=nan\n",
      "Current iteration=544, the loss=nan\n",
      "Current iteration=545, the loss=nan\n",
      "Current iteration=546, the loss=nan\n",
      "Current iteration=547, the loss=nan\n",
      "Current iteration=548, the loss=nan\n",
      "Current iteration=549, the loss=nan\n",
      "Current iteration=550, the loss=nan\n",
      "Current iteration=551, the loss=nan\n",
      "Current iteration=552, the loss=nan\n",
      "Current iteration=553, the loss=nan\n",
      "Current iteration=554, the loss=nan\n",
      "Current iteration=555, the loss=nan\n",
      "Current iteration=556, the loss=nan\n",
      "Current iteration=557, the loss=nan\n",
      "Current iteration=558, the loss=nan\n",
      "Current iteration=559, the loss=nan\n",
      "Current iteration=560, the loss=nan\n",
      "Current iteration=561, the loss=nan\n",
      "Current iteration=562, the loss=nan\n",
      "Current iteration=563, the loss=nan\n",
      "Current iteration=564, the loss=nan\n",
      "Current iteration=565, the loss=nan\n",
      "Current iteration=566, the loss=nan\n",
      "Current iteration=567, the loss=nan\n",
      "Current iteration=568, the loss=nan\n",
      "Current iteration=569, the loss=nan\n",
      "Current iteration=570, the loss=nan\n",
      "Current iteration=571, the loss=nan\n",
      "Current iteration=572, the loss=nan\n",
      "Current iteration=573, the loss=nan\n",
      "Current iteration=574, the loss=nan\n",
      "Current iteration=575, the loss=nan\n",
      "Current iteration=576, the loss=nan\n",
      "Current iteration=577, the loss=nan\n",
      "Current iteration=578, the loss=nan\n",
      "Current iteration=579, the loss=nan\n",
      "Current iteration=580, the loss=nan\n",
      "Current iteration=581, the loss=nan\n",
      "Current iteration=582, the loss=nan\n",
      "Current iteration=583, the loss=nan\n",
      "Current iteration=584, the loss=nan\n",
      "Current iteration=585, the loss=nan\n",
      "Current iteration=586, the loss=nan\n",
      "Current iteration=587, the loss=nan\n",
      "Current iteration=588, the loss=nan\n",
      "Current iteration=589, the loss=nan\n",
      "Current iteration=590, the loss=nan\n",
      "Current iteration=591, the loss=nan\n",
      "Current iteration=592, the loss=nan\n",
      "Current iteration=593, the loss=nan\n",
      "Current iteration=594, the loss=nan\n",
      "Current iteration=595, the loss=nan\n",
      "Current iteration=596, the loss=nan\n",
      "Current iteration=597, the loss=nan\n",
      "Current iteration=598, the loss=nan\n",
      "Current iteration=599, the loss=nan\n",
      "Current iteration=600, the loss=nan\n",
      "Current iteration=601, the loss=nan\n",
      "Current iteration=602, the loss=nan\n",
      "Current iteration=603, the loss=nan\n",
      "Current iteration=604, the loss=nan\n",
      "Current iteration=605, the loss=nan\n",
      "Current iteration=606, the loss=nan\n",
      "Current iteration=607, the loss=nan\n",
      "Current iteration=608, the loss=nan\n",
      "Current iteration=609, the loss=nan\n",
      "Current iteration=610, the loss=nan\n",
      "Current iteration=611, the loss=nan\n",
      "Current iteration=612, the loss=nan\n",
      "Current iteration=613, the loss=nan\n",
      "Current iteration=614, the loss=nan\n",
      "Current iteration=615, the loss=nan\n",
      "Current iteration=616, the loss=nan\n",
      "Current iteration=617, the loss=nan\n",
      "Current iteration=618, the loss=nan\n",
      "Current iteration=619, the loss=nan\n",
      "Current iteration=620, the loss=nan\n",
      "Current iteration=621, the loss=nan\n",
      "Current iteration=622, the loss=nan\n",
      "Current iteration=623, the loss=nan\n",
      "Current iteration=624, the loss=nan\n",
      "Current iteration=625, the loss=nan\n",
      "Current iteration=626, the loss=nan\n",
      "Current iteration=627, the loss=nan\n",
      "Current iteration=628, the loss=nan\n",
      "Current iteration=629, the loss=nan\n",
      "Current iteration=630, the loss=nan\n",
      "Current iteration=631, the loss=nan\n",
      "Current iteration=632, the loss=nan\n",
      "Current iteration=633, the loss=nan\n",
      "Current iteration=634, the loss=nan\n",
      "Current iteration=635, the loss=nan\n",
      "Current iteration=636, the loss=nan\n",
      "Current iteration=637, the loss=nan\n",
      "Current iteration=638, the loss=nan\n",
      "Current iteration=639, the loss=nan\n",
      "Current iteration=640, the loss=nan\n",
      "Current iteration=641, the loss=nan\n",
      "Current iteration=642, the loss=nan\n",
      "Current iteration=643, the loss=nan\n",
      "Current iteration=644, the loss=nan\n",
      "Current iteration=645, the loss=nan\n",
      "Current iteration=646, the loss=nan\n",
      "Current iteration=647, the loss=nan\n",
      "Current iteration=648, the loss=nan\n",
      "Current iteration=649, the loss=nan\n",
      "Current iteration=650, the loss=nan\n",
      "Current iteration=651, the loss=nan\n",
      "Current iteration=652, the loss=nan\n",
      "Current iteration=653, the loss=nan\n",
      "Current iteration=654, the loss=nan\n",
      "Current iteration=655, the loss=nan\n",
      "Current iteration=656, the loss=nan\n",
      "Current iteration=657, the loss=nan\n",
      "Current iteration=658, the loss=nan\n",
      "Current iteration=659, the loss=nan\n",
      "Current iteration=660, the loss=nan\n",
      "Current iteration=661, the loss=nan\n",
      "Current iteration=662, the loss=nan\n",
      "Current iteration=663, the loss=nan\n",
      "Current iteration=664, the loss=nan\n",
      "Current iteration=665, the loss=nan\n",
      "Current iteration=666, the loss=nan\n",
      "Current iteration=667, the loss=nan\n",
      "Current iteration=668, the loss=nan\n",
      "Current iteration=669, the loss=nan\n",
      "Current iteration=670, the loss=nan\n",
      "Current iteration=671, the loss=nan\n",
      "Current iteration=672, the loss=nan\n",
      "Current iteration=673, the loss=nan\n",
      "Current iteration=674, the loss=nan\n",
      "Current iteration=675, the loss=nan\n",
      "Current iteration=676, the loss=nan\n",
      "Current iteration=677, the loss=nan\n",
      "Current iteration=678, the loss=nan\n",
      "Current iteration=679, the loss=nan\n",
      "Current iteration=680, the loss=nan\n",
      "Current iteration=681, the loss=nan\n",
      "Current iteration=682, the loss=nan\n",
      "Current iteration=683, the loss=nan\n",
      "Current iteration=684, the loss=nan\n",
      "Current iteration=685, the loss=nan\n",
      "Current iteration=686, the loss=nan\n",
      "Current iteration=687, the loss=nan\n",
      "Current iteration=688, the loss=nan\n",
      "Current iteration=689, the loss=nan\n",
      "Current iteration=690, the loss=nan\n",
      "Current iteration=691, the loss=nan\n",
      "Current iteration=692, the loss=nan\n",
      "Current iteration=693, the loss=nan\n",
      "Current iteration=694, the loss=nan\n",
      "Current iteration=695, the loss=nan\n",
      "Current iteration=696, the loss=nan\n",
      "Current iteration=697, the loss=nan\n",
      "Current iteration=698, the loss=nan\n",
      "Current iteration=699, the loss=nan\n",
      "Current iteration=700, the loss=nan\n",
      "Current iteration=701, the loss=nan\n",
      "Current iteration=702, the loss=nan\n",
      "Current iteration=703, the loss=nan\n",
      "Current iteration=704, the loss=nan\n",
      "Current iteration=705, the loss=nan\n",
      "Current iteration=706, the loss=nan\n",
      "Current iteration=707, the loss=nan\n",
      "Current iteration=708, the loss=nan\n",
      "Current iteration=709, the loss=nan\n",
      "Current iteration=710, the loss=nan\n",
      "Current iteration=711, the loss=nan\n",
      "Current iteration=712, the loss=nan\n",
      "Current iteration=713, the loss=nan\n",
      "Current iteration=714, the loss=nan\n",
      "Current iteration=715, the loss=nan\n",
      "Current iteration=716, the loss=nan\n",
      "Current iteration=717, the loss=nan\n",
      "Current iteration=718, the loss=nan\n",
      "Current iteration=719, the loss=nan\n",
      "Current iteration=720, the loss=nan\n",
      "Current iteration=721, the loss=nan\n",
      "Current iteration=722, the loss=nan\n",
      "Current iteration=723, the loss=nan\n",
      "Current iteration=724, the loss=nan\n",
      "Current iteration=725, the loss=nan\n",
      "Current iteration=726, the loss=nan\n",
      "Current iteration=727, the loss=nan\n",
      "Current iteration=728, the loss=nan\n",
      "Current iteration=729, the loss=nan\n",
      "Current iteration=730, the loss=nan\n",
      "Current iteration=731, the loss=nan\n",
      "Current iteration=732, the loss=nan\n",
      "Current iteration=733, the loss=nan\n",
      "Current iteration=734, the loss=nan\n",
      "Current iteration=735, the loss=nan\n",
      "Current iteration=736, the loss=nan\n",
      "Current iteration=737, the loss=nan\n",
      "Current iteration=738, the loss=nan\n",
      "Current iteration=739, the loss=nan\n",
      "Current iteration=740, the loss=nan\n",
      "Current iteration=741, the loss=nan\n",
      "Current iteration=742, the loss=nan\n",
      "Current iteration=743, the loss=nan\n",
      "Current iteration=744, the loss=nan\n",
      "Current iteration=745, the loss=nan\n",
      "Current iteration=746, the loss=nan\n",
      "Current iteration=747, the loss=nan\n",
      "Current iteration=748, the loss=nan\n",
      "Current iteration=749, the loss=nan\n",
      "Current iteration=750, the loss=nan\n",
      "Current iteration=751, the loss=nan\n",
      "Current iteration=752, the loss=nan\n",
      "Current iteration=753, the loss=nan\n",
      "Current iteration=754, the loss=nan\n",
      "Current iteration=755, the loss=nan\n",
      "Current iteration=756, the loss=nan\n",
      "Current iteration=757, the loss=nan\n",
      "Current iteration=758, the loss=nan\n",
      "Current iteration=759, the loss=nan\n",
      "Current iteration=760, the loss=nan\n",
      "Current iteration=761, the loss=nan\n",
      "Current iteration=762, the loss=nan\n",
      "Current iteration=763, the loss=nan\n",
      "Current iteration=764, the loss=nan\n",
      "Current iteration=765, the loss=nan\n",
      "Current iteration=766, the loss=nan\n",
      "Current iteration=767, the loss=nan\n",
      "Current iteration=768, the loss=nan\n",
      "Current iteration=769, the loss=nan\n",
      "Current iteration=770, the loss=nan\n",
      "Current iteration=771, the loss=nan\n",
      "Current iteration=772, the loss=nan\n",
      "Current iteration=773, the loss=nan\n",
      "Current iteration=774, the loss=nan\n",
      "Current iteration=775, the loss=nan\n",
      "Current iteration=776, the loss=nan\n",
      "Current iteration=777, the loss=nan\n",
      "Current iteration=778, the loss=nan\n",
      "Current iteration=779, the loss=nan\n",
      "Current iteration=780, the loss=nan\n",
      "Current iteration=781, the loss=nan\n",
      "Current iteration=782, the loss=nan\n",
      "Current iteration=783, the loss=nan\n",
      "Current iteration=784, the loss=nan\n",
      "Current iteration=785, the loss=nan\n",
      "Current iteration=786, the loss=nan\n",
      "Current iteration=787, the loss=nan\n",
      "Current iteration=788, the loss=nan\n",
      "Current iteration=789, the loss=nan\n",
      "Current iteration=790, the loss=nan\n",
      "Current iteration=791, the loss=nan\n",
      "Current iteration=792, the loss=nan\n",
      "Current iteration=793, the loss=nan\n",
      "Current iteration=794, the loss=nan\n",
      "Current iteration=795, the loss=nan\n",
      "Current iteration=796, the loss=nan\n",
      "Current iteration=797, the loss=nan\n",
      "Current iteration=798, the loss=nan\n",
      "Current iteration=799, the loss=nan\n",
      "Current iteration=800, the loss=nan\n",
      "Current iteration=801, the loss=nan\n",
      "Current iteration=802, the loss=nan\n",
      "Current iteration=803, the loss=nan\n",
      "Current iteration=804, the loss=nan\n",
      "Current iteration=805, the loss=nan\n",
      "Current iteration=806, the loss=nan\n",
      "Current iteration=807, the loss=nan\n",
      "Current iteration=808, the loss=nan\n",
      "Current iteration=809, the loss=nan\n",
      "Current iteration=810, the loss=nan\n",
      "Current iteration=811, the loss=nan\n",
      "Current iteration=812, the loss=nan\n",
      "Current iteration=813, the loss=nan\n",
      "Current iteration=814, the loss=nan\n",
      "Current iteration=815, the loss=nan\n",
      "Current iteration=816, the loss=nan\n",
      "Current iteration=817, the loss=nan\n",
      "Current iteration=818, the loss=nan\n",
      "Current iteration=819, the loss=nan\n",
      "Current iteration=820, the loss=nan\n",
      "Current iteration=821, the loss=nan\n",
      "Current iteration=822, the loss=nan\n",
      "Current iteration=823, the loss=nan\n",
      "Current iteration=824, the loss=nan\n",
      "Current iteration=825, the loss=nan\n",
      "Current iteration=826, the loss=nan\n",
      "Current iteration=827, the loss=nan\n",
      "Current iteration=828, the loss=nan\n",
      "Current iteration=829, the loss=nan\n",
      "Current iteration=830, the loss=nan\n",
      "Current iteration=831, the loss=nan\n",
      "Current iteration=832, the loss=nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-df027351a3b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimal_lambda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimal_loss_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimal_loss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_parameters_Logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The optimal lambda for GD is : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimal_lambda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" ; loss_tr = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimal_loss_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" ; loss_te = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimal_loss_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hugomoreau/Dropbox/Etudes/Master/Pattern classification and machine learning/Projects/Project 1/Github-Project1-PCML/scripts/crossvalidation.py\u001b[0m in \u001b[0;36mget_best_parameters_Logistic\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mloss_te_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mloss_tr_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mloss_te_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hugomoreau/Dropbox/Etudes/Master/Pattern classification and machine learning/Projects/Project 1/Github-Project1-PCML/scripts/crossvalidation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, lambda_, model, degree, poly_built_already)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mweight_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mweight_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mweight_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hugomoreau/Dropbox/Etudes/Master/Pattern classification and machine learning/Projects/Project 1/Github-Project1-PCML/scripts/optimization.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, gamma, max_iters)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;31m#loss, w = learning_by_newton_method(y,tx,w,gamma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hugomoreau/Dropbox/Etudes/Master/Pattern classification and machine learning/Projects/Project 1/Github-Project1-PCML/scripts/optimization.py\u001b[0m in \u001b[0;36mlearning_by_gradient_descent\u001b[0;34m(y, tx, w, alpha)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \"\"\"\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hugomoreau/Dropbox/Etudes/Master/Pattern classification and machine learning/Projects/Project 1/Github-Project1-PCML/scripts/optimization.py\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimal_lambda,optimal_loss_tr,optimal_loss_te = get_best_parameters_Logistic(y,x)\n",
    "print(\"The optimal lambda for GD is : \",optimal_lambda,\" ; loss_tr = \",optimal_loss_tr,\" ; loss_te = \",optimal_loss_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
